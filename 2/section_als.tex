

\subsection{原理と方法}

\paragraph{交互最小二乗法}
非線形最小二乗問題のうち，パラメータを二分割すると片方ずつは線形になる場合に適用する．
パラメータを $\theta=(\alpha,\beta)^\top$ と分割し，$g(\alpha,\beta,x)$ が $\alpha,\beta$ に関してそれぞれ線形（双線形など）であるとする．
観測 $(x_i,y_i)$ に対し，目的関数
\begin{align}
  J(\alpha,\beta) := \sum_{i=1}^N \|\,y_i - g(\alpha,\beta,x_i)\,\|^2
\end{align}
を最小化する．片方のパラメータを固定すれば通常の線形最小二乗（式(3.5)型）で解ける．
交互更新により $J$ は単調非増加で，反復に伴い悪化しない．ただし一般には局所解に収束しうるため初期値が重要である．


初期値 $\alpha^{(0)},\beta^{(0)}$ を与え，$j=1,2,\ldots$ について次を繰り返す．
\begin{enumerate}
  \item \textbf{Step A（$\beta$ 固定）}：$\beta=\beta^{(j-1)}$ として
  \begin{align}
    \alpha^{(j)} := \arg\min_{\alpha}\ \sum_{i=1}^N \|\,y_i - g(\alpha,\beta^{(j-1)},x_i)\,\|^2.
  \end{align}
  これは線形最小二乗で解く．
  \item \textbf{Step B（$\alpha$ 固定）}：$\alpha=\alpha^{(j)}$ として
  \begin{align}
    \beta^{(j)} := \arg\min_{\beta}\ \sum_{i=1}^N \|\,y_i - g(\alpha^{(j)},\beta,x_i)\,\|^2.
  \end{align}
  これも線形最小二乗で解く．
\end{enumerate}
収束判定は例えば $\|\alpha^{(j)}-\alpha^{(j-1)}\|^2+\|\beta^{(j)}-\beta^{(j-1)}\|^2<\varepsilon$ などとする．
更新ごとに $J(\alpha^{(j)},\beta^{(j)}) \le J(\alpha^{(j-1)},\beta^{(j-1)})$ が成り立つ性質を利用する．
初期値の多点スタートや正則化の併用が実務上有効である．

以下は、交互最小二乗法のRによる実装例である。

\begin{lstlisting}[language=R,caption={交互最小二乗法更新}]
update_alpha_beta <- function(alpha, beta, x, x_T, y, n) {
    # Step 1: alphaを固定してbetaを推定
    x_alpha <- matrix(0, nrow = n, ncol = 3)
    for (i in 1:n) {
        x_alpha[i, ] <- t(alpha) %*% x[, , i]
    }
    result <- regression_simple(x_alpha, y)
    beta_new <- result$theta_hat
    
    # Step 2: betaを固定してalphaを推定
    x_beta <- matrix(0, nrow = n, ncol = 2)
    for (i in 1:n) {
        x_beta[i, ] <- t(beta_new) %*% x_T[, , i]
    }
    result2 <- regression_simple(x_beta, y)
    alpha_new <- result2$theta_hat
    
    return(list(alpha_new = alpha_new, beta_new = beta_new))
}
\end{lstlisting}




\paragraph{K-平均法}
データ $\{x_i\}_{i=1}^N \subset \mathbb{R}^p$ を $K$ 個のクラスタに分割する．
クラスタ $V_\ell$ の代表点（重心）を
\begin{align}
  \mu(V_\ell) := \arg\min_{\mu\in\mathbb{R}^p} \sum_{i\in V_\ell} \|x_i-\mu\|^2
  = \frac{1}{|V_\ell|}\sum_{i\in V_\ell} x_i
\end{align}
と定めると，目的は
\begin{align}
  \min_{\{V_\ell\}}\ \sum_{\ell=1}^K \sum_{i\in V_\ell} \|x_i-\mu(V_\ell)\|^2
\end{align}
の最小化である．指示変数 $r_{i,\ell}\in\{0,1\}$（$x_i$ がクラスタ $\ell$ に属すれば1）を用いると
\begin{align}
  \min_{\{\mu_\ell\},\{r_{i,\ell}\}}\ \sum_{\ell=1}^K \sum_{i=1}^N r_{i,\ell}\,\|x_i-\mu_\ell\|^2
\end{align}
と書ける．行列表示では $X=[x_1,\dots,x_N]$, $U=[\mu_1,\dots,\mu_K]$, $R=(r_{i,\ell})$ として
\begin{align}
  \|X-UR\|_F^2
\end{align}
の最小化に等価である．大域最適性は保証されず初期値依存性が強い．


初期重心 $\{\mu_\ell^{(0)}\}_{\ell=1}^K$ を与え，$j=1,2,\ldots$ について次を交互に実行する．
\begin{enumerate}
  \item \textbf{割当て（Assignment）}：各データを最も近い重心へ割り当てる．
  \begin{align}
    r_{i,\ell}^{(j)}=\begin{cases}
      1 & \text{if } \ell=\arg\min_{k=1,\dots,K}\ \|x_i-\mu_k^{(j-1)}\|^2,\\
      0 & \text{otherwise.}
    \end{cases}
  \end{align}
  \item \textbf{重心更新（Update）}：各クラスタの平均で重心を更新する．
  \begin{align}
    \mu_\ell^{(j)}=\frac{\sum_{i=1}^N r_{i,\ell}^{(j)}\,x_i}{\sum_{i=1}^N r_{i,\ell}^{(j)}}\quad (\sum_{i} r_{i,\ell}^{(j)} > 0).
  \end{align}
\end{enumerate}
停止条件は $\max_\ell \|\mu_\ell^{(j)}-\mu_\ell^{(j-1)}\|^2<\varepsilon$ 等とする．
目的関数は反復で非増加となるが，局所解に収束するため初期化戦略（例：多点スタート，$k$-means++）が重要である．

以下は、K-平均法のRによる実装例である。

\begin{lstlisting}[language=R,caption={K-平均法}]
k_means <- function(x, K, max_iters = 10000, tol = 1e-6) {
    n <- nrow(x)
    d <- ncol(x)
    
    # ランダムに初期クラスタ中心を選択
    centroids <- x[sample(1:n, K), ]
    
    cluster_assignments <- rep(0, n)
    for (iter in 1:max_iters) {
        # 各データポイントを最も近いクラスタ中心に割り当て
        for (i in 1:n) {
            distances <- apply(centroids, 1, function(centroid) sum((x[i, ] - centroid)^2))
            cluster_assignments[i] <- which.min(distances)
        }
        
        # 新しいクラスタ中心を計算
        new_centroids <- matrix(0, nrow = K, ncol = d)
        for (k in 1:K) {
            points_in_cluster <- x[cluster_assignments == k, , drop = FALSE]
            if (nrow(points_in_cluster) > 0) {
                new_centroids[k, ] <- colMeans(points_in_cluster)
            } else {
                new_centroids[k, ] <- centroids[k, ]  # クラスタにポイントがない場合は中心を維持
            }
        }
        
        # 収束判定
        if (max(sqrt(rowSums((new_centroids - centroids)^2))) < tol) {
            # 各データポイントから割り当てられた中心までの距離の合計を計算
            total_distance <- 0
            for (i in 1:n) {
                centroid_idx <- cluster_assignments[i]
                distance <- sum((x[i, ] - centroids[centroid_idx, ])^2)
                total_distance <- total_distance + distance
            }
            break
        }
        
        centroids <- new_centroids
    }
    
    return(list(
        centroids = centroids,
        cluster_assignments = cluster_assignments,
        total_distance = total_distance
    ))
}
\end{lstlisting}

\subsection{課題13（交互最小二乗法）}
\label{subsec:task13}

\paragraph{内容}
パラメータ $\alpha\in\mathbb{R}^{2}$, $\beta\in\mathbb{R}^{3}$を用いた双線形モデル
\[
y_i \approx \alpha^\top X_i \beta
\]
を最小二乗で推定する．特徴は $\phi_i=(1,\,x_i,\,x_i^2,\,x_i^3)$ とし，
\[
X_i=
\begin{pmatrix}
\phi_{i,1} & \phi_{i,2} & \phi_{i,3}\\
\phi_{i,2} & \phi_{i,3} & \phi_{i,4}
\end{pmatrix}
\in\mathbb{R}^{2\times 3}
\]
を用いる．目的は $\min_{\alpha,\beta}\sum_{i=1}^{n}\bigl(y_i-\alpha^\top X_i\beta\bigr)^2$．

\paragraph{方法}
交互最小二乗法（ALS）を用いる．反復 $j=1,2,\dots$ に対し
\begin{align}
\beta^{(j)}&:=\arg\min_{\beta}\sum_{i=1}^{n}\bigl(y_i-(\alpha^{(j-1)})^\top X_i\beta\bigr)^2,\\
\alpha^{(j)}&:=\arg\min_{\alpha}\sum_{i=1}^{n}\bigl(y_i-\alpha^\top X_i\beta^{(j)}\bigr)^2,
\end{align}
を交互に解く（いずれも線形最小二乗）．収束判定は
\[
\max\bigl(\|\alpha^{(j)}-\alpha^{(j-1)}\|_2,\ \|\beta^{(j)}-\beta^{(j-1)}\|_2\bigr)<\varepsilon.
\]

\paragraph{実装}
CSV \verb|datas/mmse_kadai13.csv| を読み込み，$X_i$ を \verb|x[,,i]|，$X_i^\top$ を \verb|x_T[,,i]| に格納．
\verb|update_alpha_beta| で
\[
\underbrace{(\alpha^\top X_i)_{i=1:n}}_{\text{$n\times 3$設計行列}}\ \text{から}\ \beta,
\quad
\underbrace{(X_i\beta)_{i=1:n}}_{\text{$n\times 2$設計行列}}\ \text{から}\ \alpha
\]
を逐次推定する．多点初期化（乱数）で \verb|num_trials=10| を走らせ最良解を採用．収束履歴と
$\alpha$ の軌跡を \verb|graphs/task13_convergence.png|，\verb|graphs/task13_trajectory.png| に保存する．

\paragraph{結果}
全試行 $10$ 回の平均二乗誤差（MMSE）は
\[
\mathrm{MMSE}=0.3329
\]
であった（有効数字4桁）．最良試行での推定結果は
\[
\alpha=
\begin{pmatrix}
0.6000\\[-2pt]
-1.200
\end{pmatrix},\quad
\beta=
\begin{pmatrix}
0.8441\\[-2pt]
-1.661\\[-2pt]
3.334
\end{pmatrix},
\]
\[
\theta=\alpha\beta^\top=
\left(
\begin{array}{ccc}
\ 0.5065 & -0.9967 & \ 2.000\\
-1.013 & \ 1.994 & -4.002
\end{array}
\right).
\]
収束挙動とパラメータ軌跡を図\ref{fig:task13_conv}および図\ref{fig:task13_traj}に示す．

\begin{figure}[H]
  \centering
  \includegraphics[width=0.78\linewidth]{graphs/task13_convergence.png}
  \caption{ALS による収束履歴（縦軸は $\max(\|\Delta\alpha\|_2,\|\Delta\beta\|_2)$，対数目盛）}
  \label{fig:task13_conv}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.78\linewidth]{graphs/task13_trajectory.png}
  \caption{$\alpha$ の推定軌跡（緑：初期，赤：収束点）}
  \label{fig:task13_traj}
\end{figure}

全試行の統計情報を表\ref{tab:task13_stats}に示す．

\begin{table}[H]
\centering
\caption{全試行 $10$ 回の統計情報}
\label{tab:task13_stats}
\begin{tabular}{|l|c|}
\hline
試行回数 & 10 \\
\hline
MMSE - 平均 & 0.3329 \\
\hline
MMSE - 標準偏差 & 0.0000 \\
\hline
MMSE - 最小 & 0.3329 \\
\hline
MMSE - 最大 & 0.3329 \\
\hline
\end{tabular}
\end{table}

$\theta=\alpha\beta^\top$ の各要素の範囲を表\ref{tab:task13_theta_range}に示す．

\begin{table}[H]
\centering
\caption{$\theta=\alpha\beta^\top$ の各要素の範囲}
\label{tab:task13_theta_range}
\begin{tabular}{|c|c|c|}
\hline
要素 & 最小値 & 最大値 \\
\hline
$\theta_{1,1}$ & 0.5065 & 0.5065 \\
\hline
$\theta_{1,2}$ & $-0.9967$ & $-0.9967$ \\
\hline
$\theta_{1,3}$ & 2.0003 & 2.0003 \\
\hline
$\theta_{2,1}$ & $-1.0132$ & $-1.0132$ \\
\hline
$\theta_{2,2}$ & 1.9939 & 1.9939 \\
\hline
$\theta_{2,3}$ & $-4.0017$ & $-4.0017$ \\
\hline
\end{tabular}
\end{table}



\paragraph{考察}
図\ref{fig:task13_conv}と、図\ref{fig:task13_traj} より，ALS は数十回の反復で高速に収束することが分かる．

また，表\ref{tab:task13_stats}および表\ref{tab:task13_theta_range}から，全試行にわたってMMSEや$\theta$の各要素が非常に安定していることも分かる．





